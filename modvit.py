# -*- coding: utf-8 -*-
"""ModViT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16P2dQCp1DikF7CSyImgG_l9xlcgEMYZz
"""

import math
from dataclasses import dataclass
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F


@dataclass
class ViTConfig:
    image_size_h: int = 224
    image_size_w: int = 224
    patch_size: int = 16                  # P
    in_chans: int = 3                     # C
    embed_dim: int = 768                  # D
    num_heads: int = 8                    # h (fixed to 8 per paper text)
    depth: int = 5                        # L
    mlp_ratio: float = 4.0                # hidden = mlp_ratio * D in transformer blocks
    attn_dropout: float = 0.0
    proj_dropout: float = 0.0
    mlp_dropout: float = 0.0
    resmlp_hidden: Optional[int] = None   # defaults to D if None
    resmlp_dropout: float = 0.1
    num_classes: int = 10


class PatchEmbedLinear(nn.Module):
    """
    Generation of patchs, then linear projection E \in R^{(P^2*C) x D}
    """
    def __init__(self, img_h, img_w, patch_size, in_chans, embed_dim):
        super().__init__()
        assert img_h % patch_size == 0 and img_w % patch_size == 0, "H and W must be divisible by P"
        self.img_h, self.img_w = img_h, img_w
        self.patch_size = patch_size
        self.num_patches_h = img_h // patch_size
        self.num_patches_w = img_w // patch_size
        self.num_patches = self.num_patches_h * self.num_patches_w  # N_p
        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)
        self.proj = nn.Linear((patch_size * patch_size) * in_chans, embed_dim, bias=True)

    def forward(self, x):
        # x: (B, C, H, W)
        B, C, H, W = x.shape
        patches = self.unfold(x)                        # (B, P^2*C, N_p)
        patches = patches.transpose(1, 2)               # (B, N_p, P^2*C)
        z = self.proj(patches)                          # (B, N_p, D)
        return z


class MLP(nn.Module):
    def __init__(self, dim, hidden_dim, dropout=0.0, activation=nn.GELU):
        super().__init__()
        self.fc1 = nn.Linear(dim, hidden_dim)
        self.act = activation()
        self.fc2 = nn.Linear(hidden_dim, dim)
        self.drop = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class ParallelEncoderLayer(nn.Module):
    """
    Parallel Transformer Encoder layer implementing
    Uses pre-LN, batch_first MHA.
    """
    def __init__(self, d_model, num_heads, mlp_ratio=4.0, attn_dropout=0.0, proj_dropout=0.0, mlp_dropout=0.0):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.mha1 = nn.MultiheadAttention(d_model, num_heads, dropout=attn_dropout, batch_first=True)
        self.mha2 = nn.MultiheadAttention(d_model, num_heads, dropout=attn_dropout, batch_first=True)
        self.attn_out_drop = nn.Dropout(proj_dropout)

        hidden = int(d_model * mlp_ratio)
        self.ln2 = nn.LayerNorm(d_model)
        self.mlp1 = MLP(d_model, hidden, dropout=mlp_dropout)
        self.mlp2 = MLP(d_model, hidden, dropout=mlp_dropout)

    def forward(self, x):
        # x: (B, N, D)
        z = self.ln1(x)
        a1, _ = self.mha1(z, z, z, need_weights=False)  # (B, N, D)
        a2, _ = self.mha2(z, z, z, need_weights=False)
        z_prime = self.attn_out_drop(a1 + a2) + x       # Eq. (15)

        z2 = self.ln2(z_prime)
        m1 = self.mlp1(z2)
        m2 = self.mlp2(z2)
        out = m1 + m2 + z_prime                         # Eq. (16)
        return out


class ResMLPHead(nn.Module):
    """
    Two FC layers, each followed by tanh and dropout,
    with a residual connection from input to output before the final dropout.
    """
    def __init__(self, dim, hidden=None, dropout=0.1):
        super().__init__()
        hidden = hidden or dim
        self.fc1 = nn.Linear(dim, hidden)
        self.fc2 = nn.Linear(hidden, dim)
        self.drop = nn.Dropout(dropout)

    def forward(self, x):
        # x: (B, D)
        h = torch.tanh(self.fc1(x))
        h = self.drop(h)
        h = torch.tanh(self.fc2(h))
        out = x + h                      # residual before final dropout
        out = self.drop(out)
        return out


class ViTParallel(nn.Module):
    """
    Vision Transformer with:
      - Patch embedding via Unfold + Linear (matches E in paper)
      - Learnable class token and positional embeddings
      - L Parallel Encoder layers (two MHAs in parallel, two MLPs in parallel)
      - ResMLP head with tanh & dropout, then classifier
    """
    def __init__(self, cfg: ViTConfig):
        super().__init__()
        self.cfg = cfg
        self.patch_embed = PatchEmbedLinear(
            cfg.image_size_h, cfg.image_size_w, cfg.patch_size, cfg.in_chans, cfg.embed_dim
        )
        Np = self.patch_embed.num_patches
        D = cfg.embed_dim

        # Class token and positional embeddings
        self.cls_token = nn.Parameter(torch.zeros(1, 1, D))
        self.pos_embed = nn.Parameter(torch.zeros(1, Np + 1, D))
        self.pos_drop = nn.Dropout(cfg.proj_dropout)

        # Parallel Transformer Encoder stack
        self.layers = nn.ModuleList([
            ParallelEncoderLayer(
                d_model=D,
                num_heads=cfg.num_heads,
                mlp_ratio=cfg.mlp_ratio,
                attn_dropout=cfg.attn_dropout,
                proj_dropout=cfg.proj_dropout,
                mlp_dropout=cfg.mlp_dropout,
            ) for _ in range(cfg.depth)
        ])
        self.norm = nn.LayerNorm(D)

        # ResMLP head and classifier
        self.resmlp = ResMLPHead(D, hidden=cfg.resmlp_hidden or D, dropout=cfg.resmlp_dropout)
        self.fc_out = nn.Linear(D, cfg.num_classes)

        self._init_weights()

    def _init_weights(self):
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self, x, return_probs=False):
        """
        x: (B, C, H, W)
        returns:
            logits: (B, num_classes)
            probs (optional): (B, num_classes) if return_probs=True
        """
        B = x.size(0)
        z_patches = self.patch_embed(x)                       # (B, N_p, D)
        cls = self.cls_token.expand(B, -1, -1)                # (B, 1, D)
        z0 = torch.cat([cls, z_patches], dim=1)               # (B, N_p+1, D)
        z0 = z0 + self.pos_embed                              # add positional embeddings
        z = self.pos_drop(z0)

        for layer in self.layers:
            z = layer(z)
        z = self.norm(z)

        # Take class token representation
        z_cls = z[:, 0]                                       # (B, D)

        # ResMLP head
        z_head = self.resmlp(z_cls)

        # Classifier
        logits = self.fc_out(z_head)

        if return_probs:
            probs = F.softmax(logits, dim=-1)
            return logits, probs
        return logits